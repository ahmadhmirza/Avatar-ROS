#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Jan 30 13:08:03 2020

@author: ahmad
"""


from numpy import array
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
# prepare sequence
length = 5
sample_1= array([
                [0,0,0,0],
                [255,255,255,255],
                [255,255,255,255],
                [0,0,0,0],
                ])

sample_2= array([
                [255,255,255,255],
                [0,0,0,0],
                [0,0,0,0],
                [255,255,255,255]
                ])

sample_3= array([
                [100,00,00,00],
                [255,255,255,255],
                [100,100,100,100],
                [255, 255,255,255]
                ])

sample_4= array([
                [100,255,255,100],
                [255,100,100,255],
                [100,255,100,255],
                [255,100,255,100]
                ])


result=array([1,2])

seq = array([sample_1,sample_2])

#reshape to  1-Sample, 2-TimeSteps 16-Features
def reshapeArray(array):
    x= array.reshape(1,2,16)
    return x

def reshapeArray_1(array):
    x= array.reshape(2,2,16)
    return x

seq_1 = array([sample_1,sample_2])
seq_2 = array([sample_3,sample_4])


#Reshape 4-d to 3-d
TrainingData = array([reshapeArray(seq_1),reshapeArray(seq_2)])
TrainingData=reshapeArray_1(TrainingData)

testSample = seq
COLUMN_INDEX    =   0
ROW_INDEX       =   1

SAMPLES     = testSample.shape[COLUMN_INDEX]
TIME_STEP   = testSample.shape[ROW_INDEX]
FEATURES    = 1

#seq =sample_4

#The input to LSTM must be 3-d
# To reshape seq to 3-d

# 5 samples, 1 timestep, 1 feature
#x = seq.reshape(SAMPLES, TIME_STEP, FEATURES)
# Output, 5 samples, 1 feature
y = result.reshape(SAMPLES, FEATURES)

x= TrainingData

####################################
# Network model:
# 1 input, 1 timestep,
# LSTM Layer: 5 units
# Output Layer: Fully Connected Layer - 1 output
# Fit using ADAM optimization algo, and mean squared error loss function
# batch size = number of samples:
### Avoids having to make LSTM stateful and manage state resets

# define LSTM configuration
n_neurons = length
n_batch = length
n_epoch = 1000
# create LSTM
model = Sequential()
model.add(LSTM(n_neurons, input_shape=(x.shape[1], x.shape[2])))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
print(model.summary())
# train LSTM
model.fit(x, y, epochs=n_epoch, batch_size=n_batch, verbose=2)
# evaluate
result = model.predict(x, batch_size=n_batch, verbose=0)
for value in result:
	print('%.1f' % value)